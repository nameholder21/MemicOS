#### memicos üß†üîç autointerp server

- [repo status](#repo-status)
- [what this is](#what-this-is)
- [simple non-docker setup](#simple-non-docker-setup)
- [some docker commands for reference (outdated, needs fixing)](#some-docker-commands-for-reference-outdated-needs-fixing)

## repo status

- we haven't had as much time to work on this, but we'd like to collaborate with eleuther to add more explainers, scorers, and include the openai auto-interp types.
- it would be fantastic to standardize on auto-interp formats (eg an `explainerType` should have xyz fields, a `scorerType` should have abc fields, etc)

## what this is

auto-interp explanations and scoring, using eleutherAI's [delphi](https://github.com/EleutherAI/delphi) (formerly `sae-auto-interp`)

as much as possible we try to use classes/types from the `packages/python/memicos-autointerp-client`, which is autogenerated from the openapi spec under `openapi/schemas/openapi/autointerp-server.yaml`

> ‚ö†Ô∏è **warning:** this is draft documentation. we expect to either have better readmes or use a hosted documentation website.

> ‚ö†Ô∏è **warning:** the eleuther embedding scorer uses an embedding model only supported on CUDA (it won't work on mac mps or cpu)

## simple non-docker setup

1. `poetry lock && poetry install`

2. launch local server

   ```
   # no auto-reload
   poetry run uvicorn server:app --host 0.0.0.0 --port 5003 --workers 1
   # with auto-reload
   poetry run uvicorn server:app --host 0.0.0.0 --port 5003 --workers 1 --reload
   ```

## some docker commands for reference (outdated, needs fixing)

likely we will just have these instructions in the root directory `readme` instead, and manual builds should happen the same way as we do it for `inference`.

build the image from root directory

```
# cpu
docker build --platform=linux/amd64 -t memicos-autointerp:cpu -f apps/autointerp/Dockerfile --build-arg BUILD_TYPE=nocuda .

# gpu
docker build --platform=linux/amd64 -t memicos-autointerp:gpu -f apps/autointerp/Dockerfile --build-arg BUILD_TYPE=cuda .
```

Push the image to the registry (using google cloud here)

```
# tag + push cpu
docker tag memicos-autointerp:cpu gcr.io/$(gcloud config get-value project)/memicos-autointerp:cpu
docker push gcr.io/$(gcloud config get-value project)/memicos-autointerp:cpu

# tag + push gpu
docker tag memicos-autointerp:gpu gcr.io/$(gcloud config get-value project)/memicos-autointerp:gpu
docker push gcr.io/$(gcloud config get-value project)/memicos-autointerp:gpu
```

Run the container locally

```
# replace SECRET with your own value - it must match what webapp knows, or auth will fail

# cpu
docker run -p 5003:5003 -e SECRET=[SECRET] memicos-autointerp:cpu poetry run autointerp

# gpu
docker run --gpus all -p 5003:5003 -e SECRET=[SECRET] memicos-autointerp:gpu poetry run autointerp
```

## Testing, Linting, and Formatting

This project uses [pytest](https://docs.pytest.org/en/stable/) for testing, [pyright](https://github.com/microsoft/pyright) for type-checking, and [Ruff](https://docs.astral.sh/ruff/) for formatting and linting.

If you add new code, it would be greatly appreciated if you could add tests in the `tests` directory. You can run the tests with:

```bash
make test
```

Before commiting, make sure you format the code with:

```bash
make format
```

Finally, run all CI checks locally with:

```bash
make check-ci
```

If these pass, you're good to go! Open a pull request with your changes.
